---
title: "Stats 769 Lab 6"
author: "Chase Robertson"
date: "2022-09-20"
output: html_document
---

# The Data Set


```{r}
library(MASS)
library(e1071)
library(class)
library(parallel)

set.seed(123)

bank = read.csv("bank-subset.csv", strings=TRUE)
X = model.matrix(y ~ ., data=bank)[,-1]        # design matrix, without intercept
i = 1:1000
train = data.frame(X[i,], y=bank$y[i])         # training set
test = data.frame(X[-i,], y=bank$y[-i])        # test set

summary(bank)
```


# Introduction
1. In your own words, describe briefly the data and the practical problem that is associated with the data.

The data is quite imbalanced. 

# Basic Classification Methods
2. Use the linear discrimant analysis to predict the class labels. Produce the confusion matrix and compute the misclassification rate, for both the training and test sets.

```{r}
confusion_and_rate <- function(true, predicted, display=T) {
    if (display) print(table(true, predicted))
    if (display) cat("Misclassification rate:", mean(true != predicted), fill=T)
    else mean(true != predicted)
}

lda_fit <- lda(y ~ ., train)
lda_train_pred <- predict(lda_fit, train)
confusion_and_rate(train$y, lda_train_pred$class)
```

```{r}
lda_test_pred <- predict(lda_fit, test)
confusion_and_rate(test$y, lda_test_pred$class)
```


3. Use the Naive Bayes method to predict the class labels. Produce the confusion matrix and compute the misclassification rate, for both the training and test sets.

```{r}
nb <- naiveBayes(y ~ ., train)
nb_train_pred <- predict(nb, train)
confusion_and_rate(train$y, nb_train_pred)
```

```{r}
nb_test_pred <- predict(nb, test)
confusion_and_rate(test$y, nb_test_pred)
```


4. Use the K-nearest-neighbour (KNN) method to predict the class labels, with K=1,5, respectively. Produce the confusion matrix and compute the misclassification rates, for both the training and test sets.

```{r}
for (k in c(1, 5)) {
    cat(paste("k =", k), "Train", fill=T)
    knn_train_pred <- knn(train[,-43], train[,-43], cl=train$y, k=k)
    confusion_and_rate(train$y, knn_train_pred)
    cat(paste("k =", k), "Test", fill=T)
    knn_test_pred <- knn(train[,-43], test[,-43], cl=train$y, k=k)
    confusion_and_rate(test$y, knn_test_pred)
}
```

5. Use the K-nearest-neighbour (KNN) method to predict the class labels, with K=1,2,...,30, respectively. Compute the misclassification rates, for both the training and test sets. Show the two curves for the misclassification rate versus K in one graph.

```{r}
K = 30
train_rates = 1:K
test_rates = 1:K
for (k in 1:K) {
    knn_train_pred <- knn(train[,-43], train[,-43], cl=train$y, k=k)
    train_rates[k] = confusion_and_rate(train$y, knn_train_pred, display=F)
    knn_test_pred <- knn(train[,-43], test[,-43], cl=train$y, k=k)
    test_rates[k] = confusion_and_rate(test$y, knn_test_pred, display=F)
}
```

```{r}
rates <- data.frame(train=train_rates, test=test_rates)
matplot(rates, type="l", xlab="K", ylab="Misclassification Rate")
legend("bottomright", names(rates), col=1:2, lty=1:2)
```

# Data resampling

In the following, let us consider how to use data resampling methods to find an appropriate value for a tuning parameter, here the value of K in the KNN classificaiton method.

## Cross-validation

6. Use 10-fold cross-validation, with 20 repetitions, to find an appropriate value for K in KNN from the training data only. Explain why you have used the technique of same subsamples.

```{r}
R <- 20
CV <- 10
errors <- matrix(nrow=R*CV, ncol=K)
for (r in 1:R) {
    # shuffle training set in each repetition
    j <- sample(1:nrow(train))
    for (i in 1:CV) {
        # choose 1/10th of training set for validation
        trn <- train[j %% CV != i-1, ]
        vld <- train[j %% CV == i-1, ]
        
        for (k in 1:K) {
            # fit a model with every possible K on this subsample
            pred <- knn(trn[,-43], vld[, -43], cl=trn[, 43], k=k)
            errors[CV*(r-1)+i, k] = mean(vld[,43] != pred)
        }
    }
}
```

```{r}
cv_knn <- colMeans(errors)
plot(1:K, cv_knn, type="o", xlab="K", ylab="error")
which.min(cv_knn)
```

The same subsamples technique reduces the variance of resulting Prediction Error estimates, since the random variation between validation sets is shared among all k-nn methods. Sharing subsamples also reduced the computation time, since there were fewer random subsamples necessary.

## Jackknifing and Parallel Computing

7. Use the Jackknifing technique (with a 90% for training and 10% for testing) to find an appropriate value for K in KNN from the training data only. Use R = 200 as the number of repetitions.

```{r}
R <- 200
errors <- matrix(nrow=R, ncol=K)
for (r in 1:R) {
    ind <- sample(1:nrow(train))
    trn <- train[ind %% 10 != 0, ]
    vld <- train[ind %% 10 == 0, ]
    for (k in 1:K) {
        pred <- knn(trn[,-43], vld[, -43], cl=trn[, 43], k=k)
        errors[r, k] = mean(vld[,43] != pred)
    }
}
```

```{r}
jk_knn <- colMeans(errors)
plot(jk_knn, xlab='K', ylab='error', type="o")
which.min(jk_knn)
```


8. Rewrite/reorganise your code so that each repetition can be carried out independently. Perform the Jackknifing selection of the K-value from the training set using parallel computing, with function mclapply().

```{r}
# option to parallelise by K or by repetitions
# choose to parallelise repetitions to reduce variance of means and enable ncores > nK
# parallelisation argument is random seed to ensure independence of results
jackknife <- function(data, proportion=0.1, K=1:20, R=10, seed=123) {
    set.seed(seed)
    pe <- matrix(nrow=R, ncol=length(K))
    colnames(pe) <- K
    for (r in 1:R) {
        ind <- sample(1:nrow(data))
        vld_ind <- ind[1:nrow(data)*proportion]
        trn <- data[ind[-vld_ind], ]
        vld <- data[ind[vld_ind], ]
        for (k in K) {
            pred <- knn(trn[,-43], vld[,-43], cl=trn[,43], k=k)
            pe[r, k] = mean(vld[,43] != pred)
        }
    }
    colMeans(pe)
}

l <- mclapply(X=list(seed=1:5), FUN=jackknife, data=train, proportion=0.1, K=1:20, R=5)
kerr <- colMeans(matrix(unlist(l), ncol=20))
plot(1:20, kerr)
```

```{r}
total_reps <- 50
K = 1:5
c = 5
l <- mclapply(X=list(seed=1:5), FUN=jackknife, data=train, proportion=0.1, K=K, R=total_reps/c, mc.cores=c)
kerr <- colMeans(matrix(unlist(l), ncol=length(K)))
plot(K, kerr)
```


Compare the timings, when 1, 5, 10 or 20 cores are used (so you have to do this on a VM).

```{r}
K <- 1:30
total_reps = 100
cores <- c(1, 5, 10, 20)
for (c in cores) {
    seed <- 123 * c^2
    reps_per_core <- total_reps / c
    time <- system.time(
        colMeans(matrix(unlist(mclapply(
            list(seed=seed), FUN=jackknife, data=train, proportion=0.1, 
            K=K, R=reps_per_core, mc.cores=c)), ncol=length(K)))
        )
    cat("With", c, "core(s):", fill=T)
    print(time)
}
```

9. For results to be reproducible, it is better to use random seeds. Investigate and demonstrate how this can be achieved when mclapply() is used.

```{r}
# each parallel job should be fed a distinct, but deterministic seed
# my jackknife function already handles this

# only addition is to select seeds and pass to each job
seeds <- seq(123, by=769, length.out=max(cores))
for (i in seq_along(cores)) {
    c = cores[i]
    c_seeds = seeds[1:c]
    reps_per_core <- total_reps / c
    time <- system.time(
        colMeans(matrix(unlist(mclapply(
            list(seed=c_seeds), FUN=jackknife, data=train, proportion=0.1, 
            K=K, R=reps_per_core, mc.cores=c)), ncol=length(K)))
        )
    cat("With", c, "core(s):", fill=T)
    print(time)
    cat("Seed(s) utilised by each core:", c_seeds, fill=T)
}
```

# Summary

Write a summary of the entire report.




